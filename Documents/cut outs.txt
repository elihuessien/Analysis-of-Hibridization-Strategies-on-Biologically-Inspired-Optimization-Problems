(Intro)
In times past, as part of design processes, physical prototypes had to be constructed in order to perform experiments. Progressing from then, the development of computers has provided engineers the benefit of simulation, in which the virtual models made on the computer can be used to investigate the different aspects of real systems, reducing both the time and cost required for innovation (Mirjalili, 2019c). Along with this came realisations of the similarities between computer programs and minds. “Computers can process symbolic information, can derive conclusions from premises, can store information and recall it when it is appropriate, and so on - all things that minds do” (J. F. Kennedy et al., 2001). Reasoning on the capabilities of minds to host intelligence opened a captivating direction to the possibilities for computers and motivated a great quest for Artificial Intelligence (AI). 

Over the years, many algorithms have been developed, using AI techniques, with the goal of directly finding or optimizing towards a best solution to a given problem. 

-------------------------------

(GA)
The theory is based on the observation: ‘survival of the fittest’, through which, fitter and more capable individuals of a population naturally achieve higher survival rates in their given environments, providing them longer lifespans and more opportunities to pass on their superior genetic codes to the next generation. The weaker members of the population would typically achieve lower chances to pass on their inferior genetics, eventually being completely overwritten from the genetic history by fitter candidates over the progressing generations. Darwin’s theory hinged on the concept of variation; that there is a range of differences between the genetic makeup of the individuals in the population which, when accumulated through the principles mentioned above, would be able to push organisms past the barrier of species toward something completely different, perhaps new, but ultimately better.

--------------------------------

Typically, a genetic algorithm (GA) begins with a list of chromosomes representing a population. These chromosomes are often randomized to provide unique starting points for each member of the population. Then two processes called evaluation and fitness allocation are used to award each member a measure of ‘attractiveness’ (also called fitness) in such a way that those chromosomes which represent a better solution to the target problem are given more chances to 'reproduce' than those chromosomes which are poorer solutions. The ‘attractiveness’ of any given individual is typically assigned relative to the current population. Lastly, individuals are selected to be bred based on those allocations making a ‘next generation’, and mutations are randomly assigned under an appropriately low percentage to allow more variability in the search space . As the algorithm iterates through the generations, members of the population increase in fitness until a concluding ‘best individual’ is found after the stop criteria is/are reached (Coley, 1999; Whitley, 1994). 

----------------

However, the social psychologist James Kennedy began to notice a stereotype creeping into the general understanding of AI at the time that was limiting the understanding of what AI could be. “The early AI researchers had made an important assumption, so fundamental that it was never stated explicitly nor consciously acknowledged” (J. F. Kennedy et al., 2001). AI researchers at the time understood the measure of intelligence as the ability to solve large, complex, and sometimes multipart, problems quickly. The variety in approach methods for building intelligent computer programs that find the best choice to their given problems motivated these researchers to think of a number of clever solutions. One of these solutions were the group of ‘logical shortcuts’, called heuristics, that speed up the process in a manner that was applicably reusable. However, the growing numbers of variables needed to be addressed as these programs began to be applied in a variety of fields (like image processing and speech recognition), posed a serious hinderance to their programs and the development of AI. The problem was that the understanding of AI was modelled on the vision of a single disconnected person capable of coolheadedly handling the situations posed to them using the information and logical reasoning stored in their brain. However, Kennedy et al. (2001) argued that if human intelligence was the intended model, then this model of understanding was devoid of an important behaviour involved in human reasoning and development: Socialization.

In the late 1990s to early 2000s, along with the Electrical engineers Russel C. Eberhart, Kennedy dove into the, at the time newly growing, concept of Swarm Intelligence; exploiting social behaviours by splitting the computational requirements of a system across a group, or swarm, of inter-communicating individuals. They took inspiration from the benefits found in naturally occurring swarm-like communities of organisms like fish schooling, birds flocking and bugs swarming, to produce a model algorithm which differed from the popular evolutionary computation methods of the time (J. Kennedy & Eberhart, 1995). Named Particle Swarm Optimization, this algorithm used a population of particles (named so due to their observed mosquito like appearance and behaviour), flown across a problems space, as its iterative mechanism to optimize towards solutions. 

